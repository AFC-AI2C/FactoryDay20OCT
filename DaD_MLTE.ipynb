{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20OCT Factory Day Demo\n",
    "\n",
    "Presenter: Kyle\n",
    "\n",
    "This is a demonstration notebook to illustrate the use of the MLTE library and SDMT process for Factory Day. This demo uses the \"Dogs vs. Cats\" dataset and scenario from the Factory Day negotiation exercise as guidance for the required Properties and Conditions.\n",
    "\n",
    "**MLTE Core Concepts**\n",
    "\n",
    "- Negotiate requirements for machine learning models, considering the context of the system into which the model will be integrated\n",
    "- Rigorously specify these requirements\n",
    "- Gather evidence that attests to the fact that these requirements are satisifed, in the form of _artifacts_\n",
    "- Report on the outcome of model evaluation in a simple, useful manner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Quality Attribute Scenarios\n",
    "\n",
    "Presenter: Sebastian\n",
    "\n",
    "The following is a prioritized (but limited) list of the QASs that we want to validate through the use of MLTE. The examples below relate to the hypothetical system used by the Dogs are Dumb (DaD) Task Force to detect non-compliant service members who own dogs. The system used an ML model that was trained on the cats and dogs dataset located on [Kaggle](https://www.kaggle.com/c/dogs-vs-cats/data). \n",
    "\n",
    "* **Precision**\n",
    "  * Beause the DaD cares about identifing dogs but NEVER missclassifying a cat as a dog (false positive), this model will need to have a high precision. Precision is measured as the true positive rate divided by the true positive rate times the false positive rate. \n",
    "* **Robustness - Model Robust to Noise (Image Blur)**\n",
    "  * Because the model will receive pictures taken from a device mounted on the back of a cat, they will likely be a bit blurry. The model should still be able to successfully identify dogs at the same rate as non-blurry images. Test data needs to include blurred images.  Blurred images will be created using ImageMagick. For our purposes we will test against maximum blur. Blurry images are successfully identified at rates equal to that of non-blurred images. This will be measured using the Wilcoxon Rank-Sum test, with significance at p-value <=0.05.\n",
    "* **Performance on Operational Platform**\n",
    "  * The model will need to run on devices worn on the back of cats. These are small, inexpensive devices with limited CPU power, as well as limited memory and disk space (512 MB and 128 GB, respectively). The original test dataset can be used.\n",
    "    1. Executing the model on the loaned platform will not exceed maximum CPU usage of 30% to ensure reasonable response time. CPU usage will be measure using `ps`. \n",
    "    2. Memory usage at inference time will not exceed available memory of 512 MB. This will be measured using `pmap`. \n",
    "    3. Disk usage will not exceed available disk space of 128 GB. This will be measured using by adding the size of each file in the path for the saved model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Negotiate Requirements\n",
    "\n",
    "Presenter: Kyle\n",
    "\n",
    "In the exercise, we negotiated the requirements for the model and system. MLTE provides an artifact that assists in this process - the `NegotiationCard`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Install `mlte`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install mlte package\n",
    "!pip install mlte-python==0.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Initialize MLTE Context\n",
    "\n",
    "MLTE contains a global context that manages the currently active _session_. Initializing the context tells MLTE how to store all of the artifacts that it produces. Install MLTE if not done already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlte.session import set_context, set_store\n",
    "\n",
    "store_path = os.path.join(os.getcwd(), \"store\")\n",
    "os.makedirs(\n",
    "    store_path, exist_ok=True\n",
    ")  # Ensure we are creating the folder if it is not there.\n",
    "\n",
    "set_context(\"ns\", \"DogsVCats\", \"0.0.1\")\n",
    "set_store(f\"local://{store_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Build a `NegotiationCard`\n",
    "\n",
    "The `NegotiationCard` artifact can be built via the `mlte` Python API or in the MLTE web UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.negotiation.artifact import NegotiationCard\n",
    "from mlte.model.shared import (\n",
    "    MetricDescriptor,\n",
    "    DataDescriptor,\n",
    "    DataClassification,\n",
    "    FieldDescriptor,\n",
    "    LabelDescriptor,\n",
    "    ModelDescriptor,\n",
    "    ModelDevelopmentDescriptor,\n",
    "    ModelResourcesDescriptor,\n",
    "    ModelProductionDescriptor,\n",
    "    ModelInterfaceDescriptor,\n",
    "    ModelInputDescriptor,\n",
    "    ModelOutputDescriptor,\n",
    ")\n",
    "from mlte.negotiation.model import (\n",
    "    SystemDescriptor,\n",
    "    GoalDescriptor,\n",
    "    ProblemType,\n",
    "    RiskDescriptor,\n",
    ")\n",
    "\n",
    "card = NegotiationCard(\n",
    "    system=SystemDescriptor(\n",
    "        goals=[\n",
    "            GoalDescriptor(\n",
    "                description=\"The model should precicesly identify dogs\",\n",
    "                metrics=[\n",
    "                    MetricDescriptor(\n",
    "                        description=\"Accuracy\",\n",
    "                        baseline=\"Greater than .7\",\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "        ],\n",
    "        problem_type=ProblemType.CLASSIFICATION,\n",
    "        task=\"Dog Identification\",\n",
    "        usage_context=\"A dog identification device mounted on the back of a cat.\",\n",
    "        risks=RiskDescriptor(\n",
    "            fp=\"A cat is identified as a dog; This is critical to avoid becuase an innoncent cat owning service member will be falsely convicted\",\n",
    "            fn=\"A service member owning a dog will slip through the cracks\",\n",
    "            other=\"N/A\",\n",
    "        ),\n",
    "    ),\n",
    "    data=[\n",
    "        DataDescriptor(\n",
    "            description=\"Dogs v Cats; The dataset is comprised of photos of dogs and cats provided as a subset of photos from a much larger dataset of 3 million manually annotated photos. The dataset was developed as a partnership between Petfinder.com and Microsoft.\",\n",
    "            classification=DataClassification.UNCLASSIFIED,\n",
    "            access=\"None\",\n",
    "            fields=[\n",
    "                FieldDescriptor(\n",
    "                    name=\"Filename with label cat or dog\",\n",
    "                    description=\"An image depicting a cat or a dog\",\n",
    "                    type=\"jpg\",\n",
    "                    expected_values=\"N/A\",\n",
    "                    missing_values=\"N/A\",\n",
    "                    special_values=\"N/A\",\n",
    "                )\n",
    "            ],\n",
    "            labels=[\n",
    "                LabelDescriptor(description=\"cat\", percentage=50.0),\n",
    "                LabelDescriptor(description=\"dog\", percentage=50.0),\n",
    "            ],\n",
    "            policies=\"N/A\",\n",
    "            rights=\"N/A\",\n",
    "            source=\"https://www.kaggle.com/c/dogs-vs-cats\",\n",
    "            identifiable_information=\"N/A\",\n",
    "        )\n",
    "    ],\n",
    "    model=ModelDescriptor(\n",
    "        development=ModelDevelopmentDescriptor(\n",
    "            resources=ModelResourcesDescriptor(\n",
    "                cpu=\"1\", gpu=\"0\", memory=\"512MiB\", storage=\"128GiB\"\n",
    "            )\n",
    "        ),\n",
    "        production=ModelProductionDescriptor(\n",
    "            integration=\"integration\",\n",
    "            interface=ModelInterfaceDescriptor(\n",
    "                input=ModelInputDescriptor(description=\"Vector[150]\"),\n",
    "                output=ModelOutputDescriptor(description=\"Vector[3]\"),\n",
    "            ),\n",
    "            resources=ModelResourcesDescriptor(\n",
    "                cpu=\"1\",\n",
    "                gpu=\"0\",\n",
    "                memory=\"512MiB\",\n",
    "                storage=\"128GiB\",\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "card.save(force=True, parents=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Requirements \n",
    "\n",
    "In the next phase of SDMT, we define a _Specification_ (or `Spec`) that represents the requirements the completed model must meet in order to be acceptable for use in the system into which it will be integrated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install other demonstration requirements\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Build a Specification (`Spec`)\n",
    "\n",
    "In MLTE, we define requirements by constructing a specification (`Spec`). For each property, we define the validations to perform as well. Note that several new `Value` types (`MultipleAccuracy`, `RankSums`, `MultipleRanksums`) had to be created to define the validation methods that will validate each Condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.spec.spec import Spec\n",
    "\n",
    "# The Properties we want to validate, associated with our scenarios.\n",
    "from mlte.property.functionality.task_efficacy import TaskEfficacy\n",
    "from mlte.value.types.real import Real\n",
    "from mlte.property.costs.storage_cost import StorageCost\n",
    "from properties.robustness import Robustness\n",
    "from properties.predicting_memory_cost import PredictingMemoryCost\n",
    "from properties.predicting_compute_cost import PredictingComputeCost\n",
    "\n",
    "# The Value types we will use to validate each condition.\n",
    "from mlte.measurement.storage import LocalObjectSize\n",
    "from mlte.measurement.cpu import LocalProcessCPUUtilization\n",
    "from mlte.measurement.memory import LocalProcessMemoryConsumption\n",
    "from values.confusion_matrix import ConfusionMatrix\n",
    "from values.ranksums import RankSums\n",
    "\n",
    "# The full spec.\n",
    "spec = Spec(\n",
    "    properties={\n",
    "        TaskEfficacy(\"Important to understand if the model is useful for this case\"): {\n",
    "            \"accuracy\": Real.greater_or_equal_to(0.8),\n",
    "            \"confusion matrix\": ConfusionMatrix.misclassification_count_less_than(2),\n",
    "        },\n",
    "        Robustness(\"Robust against blur\"): {\n",
    "            \"ranksums blur0x8\": RankSums.p_value_greater_or_equal_to(0.05 / 3)\n",
    "        },\n",
    "        StorageCost(\"Critical since model will be in an embedded device\"): {\n",
    "            \"model size\": LocalObjectSize.value().less_than(3000)\n",
    "        },\n",
    "        PredictingMemoryCost(\"Useful to evaluate resources needed when predicting\"): {\n",
    "            \"predicting memory\": LocalProcessMemoryConsumption.value().average_consumption_less_than(\n",
    "                512000.0\n",
    "            )\n",
    "        },\n",
    "        PredictingComputeCost(\"Useful to evaluate resources needed when predicting\"): {\n",
    "            \"predicting cpu\": LocalProcessCPUUtilization.value().max_utilization_less_than(\n",
    "                30.0\n",
    "            )\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "spec.save(parents=True, force=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude: Model Development\n",
    "\n",
    "Presenter: Kyle\n",
    "\n",
    "Before we begin the next phase of the MLTE framework, we must produce a model to use for evaluation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "This demo has an additional set of requirements in addition to MLTE. They were installed above; we import the necessary functions / modules here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from matplotlib.image import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define different folders that will be used as input or output for the data gathering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# The path at which datasets are stored\n",
    "DATASETS_DIR = Path.cwd() / \"data\"\n",
    "\n",
    "# Path where the model files are stored.\n",
    "MODELS_DIR = Path.cwd() / \"model\"\n",
    "\n",
    "# The path at which media is stored\n",
    "MEDIA_DIR = Path.cwd() / \"media\"\n",
    "os.makedirs(MEDIA_DIR, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to satisfy the requirements defined in the initial negotiation with the DaD TF, you decide to develop a convolution neural network to classify images into two categories, dog or cat. You decide to use a publicly available dataset on [Kaggle](https://www.kaggle.com/c/dogs-vs-cats/data) to train your model. The dataset is comprised of photos of dogs and cats provided as a subset of photos from a much larger dataset of 3 million manually annotated photos. The dataset was developed as a partnership between Petfinder.com and Microsoft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of \"dog\" images\n",
    "folder = \"data/train/\"\n",
    "for i in range(9):\n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    filename = folder + \"dog.\" + str(i) + \".jpg\"\n",
    "    image = imread(filename)\n",
    "    pyplot.imshow(image)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of \"cat\" images\n",
    "folder = \"data/train/\"\n",
    "for i in range(9):\n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    filename = folder + \"cat.\" + str(i) + \".jpg\"\n",
    "    image = imread(filename)\n",
    "    pyplot.imshow(image)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Explore the breakdown of train and test images\n",
    "training_images = \"data/train\"\n",
    "test_images = \"data/test\"\n",
    "\n",
    "train_size = len([name for name in os.listdir(training_images)])\n",
    "test_size = len([name for name in os.listdir(test_images)])\n",
    "\n",
    "print(\"Number of training images:\", train_size)\n",
    "print(\"Number of test images:\", test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = IMAGE_HEIGHT = 150"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare training data. Since we are developing a binary classifier, we label the target class of dogs as `1` and cats as `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Proportion of training set used for validation\n",
    "VALIDATION_FRACTION = 0.2\n",
    "\n",
    "# Training batch size (samples)\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "filenames = os.listdir(training_images)\n",
    "categories = []\n",
    "\n",
    "for filename in filenames:\n",
    "    category = filename.split(\".\")[0]\n",
    "    if category == \"dog\":\n",
    "        categories.append(1)\n",
    "    else:\n",
    "        categories.append(0)\n",
    "\n",
    "categories = [str(i) for i in categories]\n",
    "\n",
    "df = pd.DataFrame({\"filename\": filenames, \"category\": categories})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df[\"category\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split dataset into training and validation\n",
    "train_df, valid_df = train_test_split(\n",
    "    df, test_size=VALIDATION_FRACTION, random_state=10\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    rescale=1.0 / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    training_images,\n",
    "    x_col=\"filename\",\n",
    "    y_col=\"category\",\n",
    "    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "    class_mode=\"binary\",\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "valid_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "validation_generator = valid_datagen.flow_from_dataframe(\n",
    "    valid_df,\n",
    "    training_images,\n",
    "    x_col=\"filename\",\n",
    "    y_col=\"category\",\n",
    "    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "    class_mode=\"binary\",\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        # Images were resized by ImageDataGenerator 150x150 with 3 bytes color\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32, (3, 3), activation=\"relu\", input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, 3)\n",
    "        ),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.RMSprop(learning_rate=0.001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 20\n",
    "\n",
    "# The total number of validation samples\n",
    "VALIDATION_SAMPLES = int(valid_df.count()[0])\n",
    "# The total number of training samples\n",
    "TRAIN_SAMPLES = int(train_df.count()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=N_EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=VALIDATION_SAMPLES // BATCH_SIZE,\n",
    "    steps_per_epoch=TRAIN_SAMPLES // BATCH_SIZE,\n",
    ")\n",
    "\n",
    "# Serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(MODELS_DIR / \"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Serialize weights to HDF5\n",
    "model.save_weights(MODELS_DIR / \"model.h5\")\n",
    "print(\"Saved model to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the model from previous training session\n",
    "model.load_weights(MODELS_DIR / \"model.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({\"filename\": os.listdir(test_images)})\n",
    "nb_samples = test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "test_generator = test_gen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    test_images,\n",
    "    x_col=\"filename\",\n",
    "    y_col=None,\n",
    "    class_mode=None,\n",
    "    target_size=(IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df[\"probability\"] = model.predict_generator(\n",
    "    test_generator, steps=np.ceil(nb_samples / BATCH_SIZE)\n",
    ")\n",
    "test_df[\"category\"] = np.where(test_df[\"probability\"] > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_df[\"category\"].value_counts().plot.bar()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sample_test = test_df.head(5)\n",
    "\n",
    "plt.figure(figsize=(12, 24))\n",
    "for index, row in sample_test.iterrows():\n",
    "    filename = row[\"filename\"]\n",
    "    category = row[\"category\"]\n",
    "    img = load_img(\"data/test/\" + filename, target_size=(IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "    plt.subplot(5, 2, index + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.xlabel(\"Animal : \" + str(category))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df[\"probability\"] = model.predict_generator(\n",
    "    validation_generator, steps=np.ceil(nb_samples / BATCH_SIZE)\n",
    ")\n",
    "valid_df[\"predicted_label\"] = np.where(valid_df[\"probability\"] > 0.5, 1, 0)\n",
    "\n",
    "save_df = valid_df.copy()\n",
    "save_df[\"label\"] = np.where(save_df[\"filename\"].str.split(\".\").str[0] == \"dog\", 1, 0)\n",
    "save_df.drop([\"filename\", \"probability\", \"category\"], axis=1, inplace=True)\n",
    "save_df[\"model correct\"] = np.where(\n",
    "    save_df[\"label\"] == save_df[\"predicted_label\"], True, False\n",
    ")\n",
    "\n",
    "save_df.to_csv(DATASETS_DIR / \"DogsVCatsv1_ValidationResults.csv\", index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Collect Evidence\n",
    "\n",
    "Presenter: Kyle\n",
    "\n",
    "In the next phase of SDMT, we collect _evidence_ to attest to the fact that the model realized the properties specified in the previous phase.\n",
    "\n",
    "We define and instantiate `Measurement`s to generate this evidence. Each individual piece of evidence is a `Value`. Once `Value`s are produced, we can persist them to an _artifact store_ to maintain our evidence across sessions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Task Efficacy\n",
    "\n",
    "Presenter: Kyle\n",
    "\n",
    "The first set of evidence we collect relates to the model's effectiveness in accomplishing its primary function - identifying cats and dogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "from mlte.value.types.real import Real\n",
    "from mlte.measurement import ExternalMeasurement\n",
    "\n",
    "predictions = model.predict_generator(\n",
    "    validation_generator, steps=TRAIN_SAMPLES // BATCH_SIZE\n",
    ")\n",
    "predicted_classes = np.where(predictions > 0.5, 1, 0)\n",
    "true_classes = validation_generator.classes\n",
    "class_labels = list(validation_generator.class_indices.keys())\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_measurement = ExternalMeasurement(\"accuracy\", Real, metrics.accuracy_score)\n",
    "accuracy: Real = accuracy_measurement.evaluate(true_classes, predicted_classes)\n",
    "\n",
    "# Inspect value\n",
    "print(accuracy)\n",
    "\n",
    "# Save to artifact store\n",
    "accuracy.save(force=True)\n",
    "\n",
    "report = metrics.classification_report(\n",
    "    true_classes, predicted_classes, target_names=class_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from values.confusion_matrix import ConfusionMatrix\n",
    "from mlte.measurement import ExternalMeasurement\n",
    "\n",
    "# Generate value\n",
    "matrix_measurement = ExternalMeasurement(\n",
    "    \"confusion matrix\", ConfusionMatrix, confusion_matrix\n",
    ")\n",
    "matrix = matrix_measurement.evaluate(true_classes, predicted_classes)\n",
    "\n",
    "# Evaluate.\n",
    "matrix: ConfusionMatrix = matrix_measurement.evaluate(true_classes, predicted_classes)\n",
    "\n",
    "# Inspect\n",
    "print(matrix)\n",
    "\n",
    "# Save to artifact store\n",
    "matrix.save(force=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Robustness to Image Blur\n",
    "\n",
    "Presenter: Sebastian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import path\n",
    "\n",
    "\n",
    "def calculate_base_accuracy(df_results: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Calculate the base model accuracy result per data label\n",
    "    df_pos = df_results[df_results[\"model correct\"] == True].groupby(\"label\").count()\n",
    "    df_pos.drop(columns=[\"predicted_label\"], inplace=True)\n",
    "    df_neg = df_results[df_results[\"model correct\"] == False].groupby(\"label\").count()\n",
    "    df_neg.drop(columns=[\"predicted_label\"], inplace=True)\n",
    "    df_neg.rename(columns={\"model correct\": \"model incorrect\"}, inplace=True)\n",
    "    df_res = df_pos.merge(df_neg, right_on=\"label\", left_on=\"label\", how=\"outer\")\n",
    "    df_res.fillna(0, inplace=True)\n",
    "    df_res[\"model acc\"] = df_res[\"model correct\"] / (\n",
    "        df_res[\"model correct\"] + df_res[\"model incorrect\"]\n",
    "    )\n",
    "    df_res[\"count\"] = df_res[\"model correct\"] + df_res[\"model incorrect\"]\n",
    "    df_res.drop(columns=[\"model correct\", \"model incorrect\"], inplace=True)\n",
    "    df_res.head()\n",
    "\n",
    "    return df_res\n",
    "\n",
    "\n",
    "def calculate_accuracy_per_set(\n",
    "    data_folder: str, df_results: pd.DataFrame, df_res: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    # Calculate the model accuracy per data label for each blurred data set\n",
    "    base_filename = \"DogsVCatsv1_ValidationResults\"\n",
    "    ext_filename = \".csv\"\n",
    "    set_filename = [\"_blur0x8\"]\n",
    "\n",
    "    col_root = \"model acc\"\n",
    "\n",
    "    for fs in set_filename:\n",
    "        filename = os.path.join(data_folder, base_filename + fs + ext_filename)\n",
    "        colname = col_root + fs\n",
    "\n",
    "        df_temp = pd.read_csv(filename)\n",
    "        df_temp.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "\n",
    "        df_pos = df_temp[df_temp[\"model correct\"] == True].groupby(\"label\").count()\n",
    "        df_pos.drop(columns=[\"predicted_label\"], inplace=True)\n",
    "        df_neg = (\n",
    "            df_results[df_results[\"model correct\"] == False].groupby(\"label\").count()\n",
    "        )\n",
    "        df_neg.drop(columns=[\"predicted_label\"], inplace=True)\n",
    "        df_neg.rename(columns={\"model correct\": \"model incorrect\"}, inplace=True)\n",
    "        df_res2 = df_pos.merge(df_neg, right_on=\"label\", left_on=\"label\", how=\"outer\")\n",
    "        df_res2.fillna(0, inplace=True)\n",
    "\n",
    "        df_res2[colname] = df_res2[\"model correct\"] / (\n",
    "            df_res2[\"model correct\"] + df_res2[\"model incorrect\"]\n",
    "        )\n",
    "        df_res2.drop(columns=[\"model correct\", \"model incorrect\"], inplace=True)\n",
    "\n",
    "        df_res = df_res.merge(df_res2, right_on=\"label\", left_on=\"label\", how=\"outer\")\n",
    "\n",
    "    df_res.head()\n",
    "    return df_res\n",
    "\n",
    "\n",
    "def print_model_accuracy(df_res: pd.DataFrame, key: str, name: str):\n",
    "    model_acc = sum(df_res[key] * df_res[\"count\"]) / sum(df_res[\"count\"])\n",
    "    print(name, model_acc)\n",
    "\n",
    "\n",
    "def load_base_results(data_folder: str) -> pd.DataFrame:\n",
    "    df_results = pd.read_csv(\n",
    "        path.join(data_folder, \"DogsVCatsv1_ValidationResults.csv\")\n",
    "    )\n",
    "    df_results.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare all data\n",
    "df_results = load_base_results(DATASETS_DIR)\n",
    "df_res = calculate_base_accuracy(df_results)\n",
    "df_res = calculate_accuracy_per_set(DATASETS_DIR, df_results, df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View changes in model accuracy\n",
    "print_model_accuracy(df_res.head(2), \"model acc\", \"base model accuracy\")\n",
    "print_model_accuracy(\n",
    "    df_res.head(2), \"model acc_blur0x8\", \"model accuracy with 0x8 blur\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the ranksums (p-value) for all blur cases, using `scipy.stats.ranksums` and the `ExternalMeasurement` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "from values.ranksums import RankSums\n",
    "from mlte.measurement import ExternalMeasurement\n",
    "\n",
    "# Define measurements\n",
    "ranksum_measurement = ExternalMeasurement(\n",
    "    f\"ranksums blur0x8\", RankSums, scipy.stats.ranksums\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "ranksum: RankSums = ranksum_measurement.evaluate(\n",
    "    df_res[\"model acc\"], df_res[f\"model acc_blur0x8\"]\n",
    ")\n",
    "\n",
    "# Inspect values\n",
    "print(ranksum)\n",
    "\n",
    "# Save to artifact store\n",
    "ranksum.save(force=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Performance on Operational Platform\n",
    "\n",
    "Presenter: Sebastian\n",
    "\n",
    "Now we collect storage, CPU, and memory usage data when predicting with the model, for the operational performance scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the external script that will load and run the model for inference/prediction.\n",
    "script = Path.cwd() / \"model_predict.py\"\n",
    "args = [\n",
    "    \"--images\",\n",
    "    \"data/test_small\",\n",
    "    \"--model\",\n",
    "    MODELS_DIR / \"model.json\",\n",
    "    \"--weights\",\n",
    "    MODELS_DIR / \"model.h5\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.measurement.storage import LocalObjectSize\n",
    "from mlte.value.types.integer import Integer\n",
    "\n",
    "# Measure the size of the model\n",
    "store_measurement = LocalObjectSize(\"model size\")\n",
    "size: Integer = store_measurement.evaluate(MODELS_DIR)\n",
    "\n",
    "print(size)\n",
    "\n",
    "size.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.measurement import ProcessMeasurement\n",
    "from mlte.measurement.cpu import LocalProcessCPUUtilization, CPUStatistics\n",
    "\n",
    "# Measure CPU utilization during inference\n",
    "cpu_measurement = LocalProcessCPUUtilization(\"predicting cpu\")\n",
    "cpu_stats: CPUStatistics = cpu_measurement.evaluate(\n",
    "    ProcessMeasurement.start_script(script, args)\n",
    ")\n",
    "\n",
    "print(cpu_stats)\n",
    "\n",
    "cpu_stats.save(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.measurement.memory import LocalProcessMemoryConsumption, MemoryStatistics\n",
    "\n",
    "# Measure memory consumption during inference\n",
    "mem_measurement = LocalProcessMemoryConsumption(\"predicting memory\")\n",
    "mem_stats: MemoryStatistics = mem_measurement.evaluate(\n",
    "    ProcessMeasurement.start_script(script, args)\n",
    ")\n",
    "\n",
    "print(mem_stats)\n",
    "\n",
    "mem_stats.save(force=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Report Generation\n",
    "\n",
    "Presenter: Kyle\n",
    "\n",
    "The final phase of SDMT involves aggregating evidence, validating the metrics reflected by the evidence we collected, and displaying this information in a report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# The path at which reports are stored\n",
    "REPORTS_DIR = Path(os.getcwd()) / \"reports\"\n",
    "os.makedirs(REPORTS_DIR, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Validate Values\n",
    "\n",
    "Presenter: Sebastian\n",
    "\n",
    "Now that we have our `Spec` ready and we have enough evidence, we create a `SpecValidator` with our spec, and add all the `Value`s we have. With that we can validate our spec and generate an output `ValidatedSpec`, with the validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.spec.spec import Spec\n",
    "from mlte.validation.spec_validator import SpecValidator\n",
    "\n",
    "from mlte.measurement.cpu import CPUStatistics\n",
    "from mlte.measurement.memory import MemoryStatistics\n",
    "from mlte.value.types.image import Image\n",
    "from mlte.value.types.integer import Integer\n",
    "\n",
    "from values.ranksums import RankSums\n",
    "\n",
    "# Load the specification\n",
    "spec = Spec.load()\n",
    "\n",
    "# Add all values to the validator\n",
    "spec_validator = SpecValidator(spec)\n",
    "spec_validator.add_value(Real.load(\"accuracy.value\"))\n",
    "spec_validator.add_value(ConfusionMatrix.load(\"confusion matrix.value\"))\n",
    "spec_validator.add_value(RankSums.load(\"ranksums blur0x8.value\"))\n",
    "spec_validator.add_value(Integer.load(\"model size.value\"))\n",
    "spec_validator.add_value(CPUStatistics.load(\"predicting cpu.value\"))\n",
    "spec_validator.add_value(MemoryStatistics.load(\"predicting memory.value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate requirements and get validated details.\n",
    "validated_spec = spec_validator.validate()\n",
    "validated_spec.save(force=True)\n",
    "\n",
    "# We want to see the validation results in the notebook, despite the fact they are saved\n",
    "validated_spec.print_results()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Generate a Report\n",
    "\n",
    "Presenter: Kyle\n",
    "\n",
    "The final step of SDMT involves the generation of a report to communicate the results of model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlte.model.shared import (\n",
    "    ProblemType,\n",
    "    GoalDescriptor,\n",
    "    MetricDescriptor,\n",
    "    ModelProductionDescriptor,\n",
    "    ModelInterfaceDescriptor,\n",
    "    ModelInputDescriptor,\n",
    "    ModelOutputDescriptor,\n",
    "    ModelResourcesDescriptor,\n",
    "    RiskDescriptor,\n",
    "    DataDescriptor,\n",
    "    DataClassification,\n",
    "    FieldDescriptor,\n",
    "    LabelDescriptor,\n",
    ")\n",
    "from mlte.report.artifact import (\n",
    "    Report,\n",
    "    SummaryDescriptor,\n",
    "    PerformanceDesciptor,\n",
    "    IntendedUseDescriptor,\n",
    "    CommentDescriptor,\n",
    "    QuantitiveAnalysisDescriptor,\n",
    ")\n",
    "\n",
    "report = Report(\n",
    "    summary=SummaryDescriptor(\n",
    "        problem_type=ProblemType.CLASSIFICATION, task=\"Dog classification\"\n",
    "    ),\n",
    "    performance=PerformanceDesciptor(\n",
    "        goals=[\n",
    "            GoalDescriptor(\n",
    "                description=\"The model should precicesly identify dogs\",\n",
    "                metrics=[\n",
    "                    MetricDescriptor(\n",
    "                        description=\"accuracy\",\n",
    "                        baseline=\"Greater than .7\",\n",
    "                    )\n",
    "                ],\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    intended_use=IntendedUseDescriptor(\n",
    "        usage_context=\"A dog identification device worn on the back of a cat\",\n",
    "        production_requirements=ModelProductionDescriptor(\n",
    "            integration=\"integration\",\n",
    "            interface=ModelInterfaceDescriptor(\n",
    "                input=ModelInputDescriptor(description=\"Vector[150]\"),\n",
    "                output=ModelOutputDescriptor(description=\"Vector[3]\"),\n",
    "            ),\n",
    "            resources=ModelResourcesDescriptor(\n",
    "                cpu=\"1\", gpu=\"0\", memory=\"512MiB\", storage=\"128KiB\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    risks=RiskDescriptor(\n",
    "        fp=\"A cat is identified as a dog; This is critical to avoid becuase an innoncent cat owning service member will be falsely convicted\",\n",
    "        fn=\"A service member owning a dog will slip through the cracks\",\n",
    "        other=\"N/A\",\n",
    "    ),\n",
    "    data=[\n",
    "        DataDescriptor(\n",
    "            description=\"Dogs v Cats; The dataset is comprised of photos of dogs and cats provided as a subset of photos from a much larger dataset of 3 million manually annotated photos. The dataset was developed as a partnership between Petfinder.com and Microsoft.\",\n",
    "            classification=DataClassification.UNCLASSIFIED,\n",
    "            access=\"None\",\n",
    "            fields=[\n",
    "                FieldDescriptor(\n",
    "                    name=\"Filename with label cat or dog\",\n",
    "                    description=\"An image depicting a cat or a dog\",\n",
    "                    type=\"jpg\",\n",
    "                    expected_values=\"N/A\",\n",
    "                    missing_values=\"N/A\",\n",
    "                    special_values=\"N/A\",\n",
    "                )\n",
    "            ],\n",
    "            labels=[\n",
    "                LabelDescriptor(description=\"cat\", percentage=50.0),\n",
    "                LabelDescriptor(description=\"dog\", percentage=50.0),\n",
    "            ],\n",
    "            policies=\"N/A\",\n",
    "            rights=\"N/A\",\n",
    "            source=\"https://www.kaggle.com/c/dogs-vs-cats\",\n",
    "            identifiable_information=\"N/A\",\n",
    "        )\n",
    "    ],\n",
    "    comments=[\n",
    "        CommentDescriptor(\n",
    "            content=\"This model should not be used for nefarious purposes.\"\n",
    "        )\n",
    "    ],\n",
    "    quantitative_analysis=QuantitiveAnalysisDescriptor(content=\"Insert graph here.\"),\n",
    "    validated_spec_id=validated_spec.identifier,\n",
    ")\n",
    "\n",
    "report.save(force=True, parents=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
